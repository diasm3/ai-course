{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ccb79df",
   "metadata": {},
   "source": [
    "# EXAONE-3.5-2.4B-Instruct Model Exploration\n",
    "\n",
    "This notebook demonstrates how to use the EXAONE-3.5-2.4B-Instruct model from LG AI Research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9796a0",
   "metadata": {},
   "source": [
    "## Introduction to EXAONE 3.5\n",
    "\n",
    "EXAONE 3.5 is a collection of instruction-tuned bilingual (English and Korean) generative models ranging from 2.4B to 32B parameters, developed and released by LG AI Research. This series includes:\n",
    "\n",
    "1. **2.4B model** - Optimized for deployment on small or resource-constrained devices\n",
    "2. **7.8B model** - Matches the size of its predecessor but offers improved performance\n",
    "3. **32B model** - Delivers powerful performance\n",
    "\n",
    "All models support long-context processing of up to 32K tokens and demonstrate state-of-the-art performance in real-world use cases and long-context understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0902f9",
   "metadata": {},
   "source": [
    "## EXAONE-3.5-2.4B-Instruct Model Specifications\n",
    "\n",
    "- **Parameters**: 2.14B (without embeddings), 2.4B total\n",
    "- **Layers**: 30\n",
    "- **Attention Heads**: GQA with 32 Q-heads and 8 KV-heads\n",
    "- **Vocab Size**: 102,400\n",
    "- **Context Length**: 32,768 tokens\n",
    "- **Tie Word Embeddings**: True (unlike 7.8B and 32B models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ae09a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct:\n",
      "- modeling_exaone.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "Fetching 2 files: 100%|██████████| 2/2 [07:50<00:00, 235.49s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.81s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"LGAI-EXAONE/EXAONE-3.5-2.4B-Instruct\"\n",
    "\n",
    "# 인터넷 연결 시 최초 다운로드 → 이후 캐시에서 불러옴\n",
    "# trust_remote_code=True is required for models with custom code\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a2fa70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained(\"./local/exaone\")\n",
    "model.save_pretrained(\"./local/exaone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40c029",
   "metadata": {},
   "source": [
    "## Using the Model for Text Generation\n",
    "\n",
    "EXAONE 3.5 instruction-tuned language models were trained to utilize system prompts. Below is an example of how to use the model for conversational inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c2a84a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\"\n",
    "n8n이 방금 네이티브 MCP 트리거와 AI 에이전트 툴을 출시했습니다 [공식 안내]\n",
    "안녕하세요 플로우그래머 여러분! n8n이 방금 네이티브 MCP 서버와 MCP 클라이언트 노드를 출시했습니다. 이 영상에서는 MCP가 무엇인지, 왜 여러분에게 관련이 있는지 빠르게 설명한 다음, n8n 내에서 새로운 MCP 서버와 MCP 클라이언트 노드를 어떻게 사용하는지 살펴보겠습니다. 이미 MCP가 무엇인지 알고 계시다면 챕터를 사용하여 건너뛰세요.\n",
    "MCP는 Model Context Protocol의 약자입니다. 이는 인기 있는 Claude 모델(Sonnet 3.7 포함)의 제작자인 Anthropic이 설계하고 출시한 것으로, LLM이 다른 시스템과 통신하는 방식을 표준화하려는 시도입니다. 2024년 말에 출시된 이후 많은 채택이 이루어졌으며, 특히 OpenAI가 이 표준을 지원하기로 서명했고, 다양한 SaaS 앱과 데스크톱 도구들도 MCP 지원을 도입하고 있습니다.\n",
    "물론 이것이 중요한 이유는 프로토콜이 제공할 수 있는 유용성이 네트워크 효과에 크게 달려 있기 때문입니다. 사람들이 채택하지 않으면 아무도 사용하지 않을 것입니다. 예를 들어, SD 카드는 모든 사람이 사용했기 때문에 유용했습니다. 소니의 메모리 스틱은 다른 이야기죠.\n",
    "MCP가 무엇인지 깊이 기술적인 내용을 설명하는 영상은 많이 있으니, 저는 정말 높은 수준에서 MCP 프레임워크의 주요 개념적 요소를 설명하겠습니다.\n",
    "첫 번째 관련 요소는 MCP 호스트입니다. 이는 외부 세계나 시스템으로부터 컨텍스트가 필요하거나 이러한 시스템과 상호 작용(도구로 사용)해야 하는 LLM 기반 앱입니다. 예를 들어 Claude 데스크톱이 MCP 호스트의 예입니다.\n",
    "다음은 MCP 클라이언트입니다. 이는 호스트와 서버 간의 연결을 관리합니다. 이것이 우리를 MCP 서버로 이끕니다. MCP 서버는 일반적으로 다양한 기능과 서버에서 수행할 수 있는 작업을 노출하는 경량 프로그램입니다.\n",
    "제가 이해하기로는(그리고 많은 사람들이 아직 이해하려고 노력하는 중), MCP 서버는 기본적으로 API와 문서를 LLM인 MCP 호스트에 보내는 것으로 생각합니다. 이렇게 하면 단일 패키지를 받게 되어 무엇을 사용할 수 있는지, 어떻게 사용하는지, 그리고 사용할 수 있는 능력까지 알게 됩니다. 따라서 LLM의 맥락에서 실행 가능한 API라고 볼 수 있습니다.\n",
    "n8n 내의 새로운 MCP 기능에 들어가기 전에, MCP는 새로운 프로토콜이며 엔지니어링 커뮤니티의 모든 사람이 만족하는 것은 아니라는 점을 언급하는 것이 중요합니다. 마이크로서비스의 경우처럼, 모든 엔지니어가 어떤 것에 동의하도록 만들 수는 없을 것입니다. 제가 들은 좋은 주장은 기본적으로 \"컴퓨터가 서로 상호 작용하기 위해 이미 많은 잘 확립된 프로토콜(예: RESTful API)이 있는데, 왜 새로운 프로토콜을 만들어야 했는가?\"입니다. 이는 타당한 주장입니다.\n",
    "그럼에도 불구하고, 이는 AI 업계 전반에서 채택되고 있으며, 주요 기업들이 이를 채택하고 있습니다. n8n 팀은 n8n 커뮤니티가 최신 기술을 탐색하고 이러한 새로운 프로토콜이 우리가 더 많은 일을 할 수 있도록 도울 수 있는 방법을 확인할 수 있도록 이를 추가했습니다.\n",
    "우리 모두가 묻고 있는 큰 질문은, 그리고 커뮤니티의 도움을 원하는 것은, \"왜 MCP는 단순히 또 다른 REST API가 아닌가요? MCP는 n8n 워크플로우의 맥락에서 이전에 할 수 없었던 어떤 일을 할 수 있게 해주나요?\" 이 질문은 n8n 캔버스로 넘어가 n8n 내의 새로운 MCP 서버와 MCP 클라이언트 노드를 확인하기에 완벽한 시기입니다.\n",
    "먼저 MCP 서버 사용법을 보여드리겠습니다. 이를 통해 Claude 데스크톱이나 다른 MCP 호스트가 n8n 내에서 구축할 수 있는 수백 가지의 다양한 도구와 사용자 지정 워크플로우에 액세스할 수 있습니다. 이는 매우 강력할 것입니다.\n",
    "빈 워크플로우에서 첫 번째 단계를 추가해 보겠습니다. MCP 서버는 MCP 호스트가 소비할 것이므로 n8n에서는 트리거가 됩니다. MCP를 검색한 다음 MCP 서버 트리거를 선택하여 캔버스에 추가합니다. 현재는 매개변수가 없습니다. 이는 베타 버전이며, MCP가 지원하는 다른 기능들을 추가할 예정입니다. n8n 팀은 피드백을 받아 더 빠르게 개선하기 위해 이를 출시했습니다. 피드백이 있으시면 community.n8n.io에 가서 의견을 남겨주세요.\n",
    "MCP 서버를 추가했으니, 이제 호스트가 액세스할 수 있는 몇 가지 도구를 제공해야 합니다. 첫 번째 예시로 계산기 도구만 추가해 보겠습니다. LLM은 수학을 잘 하지 못하므로 도구를 사용하는 것이 좋습니다. 워크플로우를 저장하고 활성화시켜야 합니다.\n",
    "다음으로 Claude나 다른 MCP 호스트로 이동해야 합니다. 여기서는 Claude 데스크톱 앱을 사용하겠습니다. Claude 데스크톱 앱이 없다면, 필요한 것은 Claude 데스크톱 앱을 다운로드하고, Anthropic 계정이 있어야 합니다. 또한 Node.js가 설치되어 있어야 합니다. Claude 데스크톱은 현재 SSE를 통한 MCP 클라이언트와의 통신을 지원하지 않기 때문에(n8n 팀이 MCP 트리거에서 지원하는 것) 게이트웨이를 사용해야 합니다. 이 게이트웨이는 기본적으로 SSE 부분을 처리하고 Claude가 MCP 트리거와 통신할 수 있도록 합니다.\n",
    "Claude 데스크톱을 열면, 개발자 모드인지 확인하세요. 상단 메뉴에서 도움말 섹션을 찾아 개발자 모드 활성화를 클릭하세요. Claude에서 설정을 열고 개발자 섹션으로 이동한 다음 \"Edit Config\" 버튼을 클릭하세요. 이렇게 하면 폴더가 열리고 JSON 파일이 보일 것입니다. 이 파일을 텍스트 편집기로 열어주세요.\n",
    "이전에 MCP를 설정하지 않았다면 이 페이지는 비어 있을 것입니다. 여기에 몇 가지 내용을 붙여넣어야 합니다. 저는 여기서 Super Gateway를 사용하고 있습니다. 이것은 Claude가 SSE 프로토콜을 통해 MCP 트리거와 통신할 수 있게 해주는데, Claude가 현재 이를 기본적으로 지원하지 않기 때문입니다. 그래서 이것이 의존성입니다.\n",
    "하나의 팁은, 이것이 실패하거나 오류가 발생한다면, 이 명령을 터미널에서 수동으로 실행해보세요. 'npx super-gateway -s'를 입력하고 웹훅 URL을 전달하세요. 저는 이 부분에서 오류가 몇 번 발생했고, 실제로 Node.js 자체에 권한 오류가 있었습니다. 그것을 수정한 후 작동했습니다.\n",
    "여기 구조는 MCP 서버가 있고, 여러 개의 MCP 서버를 가질 수 있습니다. 이 경우 웹훅 URL을 붙여넣어야 합니다. 워크플로우로 돌아가서 웹훅 URL을 복사하고 여기에 붙여넣으세요. 저장한 다음, Claude를 재시작해야 합니다. Command+Q를 눌러 닫고 다시 엽니다.\n",
    "이제 한 개의 MCP 도구가 있고, 계산기 도구가 n8n 서버에서 제공되는 것을 볼 수 있습니다. 서버를 계산기라고 불렀을 수도 있지만, n8n 인스턴스가 있고 Claude에게 몇 가지 도구에 대한 액세스 권한을 부여한다고 가정해 보겠습니다. 이 도구는 계산기라고 합니다. 실행해 볼까요? \"50 * 10은 얼마인가요? 도구를 사용하세요.\"라고 질문합니다.\n",
    "허가 확인이 나타납니다. 사용할 인수를 보여주는 것까지 보여주니 흥미롭네요. 이 채팅에 대해 허용해 보겠습니다. 도구와 상호 작용하는 방식의 사용자 경험이 정말 멋지네요. 응답은 500입니다. 워크플로우에서 실행을 확인해 보겠습니다. 계산기를 열어보면 500이라는 응답이 보입니다. 쿼리는 50 * 10이었고 완벽합니다.\n",
    "이것은 단순한 계산기 도구이며, Claude에도 자체 계산기가 있을 텐데 그렇게 흥미롭지 않을 수 있습니다. 하지만 제가 보여드리려는 것은 이것을 설정하는 방법입니다. 실제로 이것을 만드는 데 약간의 시간이 걸렸고 몇 번의 좌절을 겪었기 때문에 여러분이 그런 경험을 피할 수 있기를 바랍니다.\n",
    "YouTube에서 이 영상을 보시는 분들을 위해 설명란에 이 코드 스니펫을 넣어두겠습니다. 다시 한번 언급하자면, 여기에 여러 도구를 추가할 수 있습니다. 다양한 벡터 저장소를 사용할 수 있으므로 Claude 데스크톱이나 다른 호스트가 다양한 앱 및 서비스와 연결되는 지식 RAG 어시스턴트가 될 수 있습니다.\n",
    "언급하고 싶은 정말 멋진 것 중 하나는 n8n 워크플로우 도구입니다. 이를 통해 MCP 호스트가 임의의 n8n 워크플로우와 상호 작용할 수 있습니다. 이 워크플로우 자체가 AI 에이전트일 수도 있고, 격리된 자체 호스팅 기업 정보일 수도 있습니다. 이것이 n8n에서 MCP 서버 트리거를 설정하는 방법입니다.\n",
    "이제 AI 에이전트에서 도구를 클릭하여 캔버스에 추가해 보겠습니다. MCP를 검색하면 MCP 클라이언트 도구가 보입니다. 이를 추가해 보겠습니다.\n",
    "맥락을 이해하기 위해, 이 경우 AI 에이전트는 호스트입니다. 호스트로서 클라이언트와 상호 작용하게 됩니다. 클라이언트에서는 MCP 서버에 대한 연결을 구성할 것입니다. 따라서 AI 에이전트는 호스트이며, 클라이언트를 사용하여 MCP 서버와 상호 작용합니다.\n",
    "방금 다른 워크플로우에서 MCP 트리거를 통해 MCP 서버를 구축했으니, 여기 AI 에이전트가 해당 계산기 도구와 상호 작용하도록 해보겠습니다. 더블 클릭하고 연결할 자격 증명을 추가합니다. 새로운 자격 증명을 만들고, 자격 증명 내부의 SSE 엔드포인트는 MCP 트리거에서 가져와야 합니다. MCP 트리거로 이동하여 프로덕션 URL을 복사하고 SSE 엔드포인트에 붙여넣은 다음 저장합니다.\n",
    "연결 테스트가 성공했습니다. 이제 설정이 완료되었으니 자격 증명을 닫겠습니다. MCP 서버는 여러 도구를 제공할 수 있으므로, 서버가 제공하는 모든 도구를 가져오거나 선택적으로 선택할 수 있습니다. 이 경우, 시스템과의 연결이 설정되었음을 알 수 있습니다. 계산기가 표시되니까요.\n",
    "이제 워크플로우 캔버스로 돌아가서 수동 채팅을 열어 테스트해 보겠습니다. \"15 * 10을 계산해보세요\"라고 요청합니다. 도구를 사용했네요. 로그를 확인해 보면 쿼리를 보냈고 응답을 받았습니다. 이제 트리거로 이동하여 실행을 확인해 보면 잘 실행되었음을 알 수 있습니다.\n",
    "이렇게 n8n 내의 AI 에이전트에서 MCP 서버와 함께 MCP 클라이언트를 설정하는 방법입니다.\n",
    "방금 MCP가 무엇인지, n8n에서 네이티브 MCP 서버를 설정하는 방법, 그리고 다른 MCP 서버와 연결할 수 있는 MCP 도구(MCP 클라이언트)를 설정하는 방법을 설명했습니다.\n",
    "다음 단계는 n8n 버전을 업데이트하고 즐겁게 사용해 보는 것입니다. 아직 n8n에 가입하지 않았다면, 제 쿠폰 코드 MAX50을 사용하세요. 클라우드 평가판 이후에 n8n 클라우드를 1년 동안 50% 할인받을 수 있습니다. n8n을 자체 호스팅할 수도 있지만, 클라우드 옵션을 선택하면 제 상사가 정말 기뻐할 것입니다.\n",
    "이 영상이 유용했기를 바랍니다. 더 보고 싶은 내용이나 제가 놓친 것 같은 점에 대해 피드백을 주세요. 이 주제에 대한 마지막 영상은 아닐 것 같습니다.\n",
    "시청해 주셔서 감사합니다. 정말 감사합니다. 저는 오리지널 플로우그래머 Max입니다. 이 영상을 시청해 주셔서 감사하며, 즐거운 플로잉 되세요!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3725639e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[|system|]You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.[|endofturn|]\n",
      "[|user|]\n",
      "n8n이 방금 네이티브 MCP 트리거와 AI 에이전트 툴을 출시했습니다 [공식 안내]\n",
      "안녕하세요 플로우그래머 여러분! n8n이 방금 네이티브 MCP 서버와 MCP 클라이언트 노드를 출시했습니다. 이 영상에서는 MCP가 무엇인지, 왜 여러분에게 관련이 있는지 빠르게 설명한 다음, n8n 내에서 새로운 MCP 서버와 MCP 클라이언트 노드를 어떻게 사용하는지 살펴보겠습니다. 이미 MCP가 무엇인지 알고 계시다면 챕터를 사용하여 건너뛰세요.\n",
      "MCP는 Model Context Protocol의 약자입니다. 이는 인기 있는 Claude 모델(Sonnet 3.7 포함)의 제작자인 Anthropic이 설계하고 출시한 것으로, LLM이 다른 시스템과 통신하는 방식을 표준화하려는 시도입니다. 2024년 말에 출시된 이후 많은 채택이 이루어졌으며, 특히 OpenAI가 이 표준을 지원하기로 서명했고, 다양한 SaaS 앱과 데스크톱 도구들도 MCP 지원을 도입하고 있습니다.\n",
      "물론 이것이 중요한 이유는 프로토콜이 제공할 수 있는 유용성이 네트워크 효과에 크게 달려 있기 때문입니다. 사람들이 채택하지 않으면 아무도 사용하지 않을 것입니다. 예를 들어, SD 카드는 모든 사람이 사용했기 때문에 유용했습니다. 소니의 메모리 스틱은 다른 이야기죠.\n",
      "MCP가 무엇인지 깊이 기술적인 내용을 설명하는 영상은 많이 있으니, 저는 정말 높은 수준에서 MCP 프레임워크의 주요 개념적 요소를 설명하겠습니다.\n",
      "첫 번째 관련 요소는 MCP 호스트입니다. 이는 외부 세계나 시스템으로부터 컨텍스트가 필요하거나 이러한 시스템과 상호 작용(도구로 사용)해야 하는 LLM 기반 앱입니다. 예를 들어 Claude 데스크톱이 MCP 호스트의 예입니다.\n",
      "다음은 MCP 클라이언트입니다. 이는 호스트와 서버 간의 연결을 관리합니다. 이것이 우리를 MCP 서버로 이끕니다. MCP 서버는 일반적으로 다양한 기능과 서버에서 수행할 수 있는 작업을 노출하는 경량 프로그램입니다.\n",
      "제가 이해하기로는(그리고 많은 사람들이 아직 이해하려고 노력하는 중), MCP 서버는 기본적으로 API와 문서를 LLM인 MCP 호스트에 보내는 것으로 생각합니다. 이렇게 하면 단일 패키지를 받게 되어 무엇을 사용할 수 있는지, 어떻게 사용하는지, 그리고 사용할 수 있는 능력까지 알게 됩니다. 따라서 LLM의 맥락에서 실행 가능한 API라고 볼 수 있습니다.\n",
      "n8n 내의 새로운 MCP 기능에 들어가기 전에, MCP는 새로운 프로토콜이며 엔지니어링 커뮤니티의 모든 사람이 만족하는 것은 아니라는 점을 언급하는 것이 중요합니다. 마이크로서비스의 경우처럼, 모든 엔지니어가 어떤 것에 동의하도록 만들 수는 없을 것입니다. 제가 들은 좋은 주장은 기본적으로 \"컴퓨터가 서로 상호 작용하기 위해 이미 많은 잘 확립된 프로토콜(예: RESTful API)이 있는데, 왜 새로운 프로토콜을 만들어야 했는가?\"입니다. 이는 타당한 주장입니다.\n",
      "그럼에도 불구하고, 이는 AI 업계 전반에서 채택되고 있으며, 주요 기업들이 이를 채택하고 있습니다. n8n 팀은 n8n 커뮤니티가 최신 기술을 탐색하고 이러한 새로운 프로토콜이 우리가 더 많은 일을 할 수 있도록 도울 수 있는 방법을 확인할 수 있도록 이를 추가했습니다.\n",
      "우리 모두가 묻고 있는 큰 질문은, 그리고 커뮤니티의 도움을 원하는 것은, \"왜 MCP는 단순히 또 다른 REST API가 아닌가요? MCP는 n8n 워크플로우의 맥락에서 이전에 할 수 없었던 어떤 일을 할 수 있게 해주나요?\" 이 질문은 n8n 캔버스로 넘어가 n8n 내의 새로운 MCP 서버와 MCP 클라이언트 노드를 확인하기에 완벽한 시기입니다.\n",
      "먼저 MCP 서버 사용법을 보여드리겠습니다. 이를 통해 Claude 데스크톱이나 다른 MCP 호스트가 n8n 내에서 구축할 수 있는 수백 가지의 다양한 도구와 사용자 지정 워크플로우에 액세스할 수 있습니다. 이는 매우 강력할 것입니다.\n",
      "빈 워크플로우에서 첫 번째 단계를 추가해 보겠습니다. MCP 서버는 MCP 호스트가 소비할 것이므로 n8n에서는 트리거가 됩니다. MCP를 검색한 다음 MCP 서버 트리거를 선택하여 캔버스에 추가합니다. 현재는 매개변수가 없습니다. 이는 베타 버전이며, MCP가 지원하는 다른 기능들을 추가할 예정입니다. n8n 팀은 피드백을 받아 더 빠르게 개선하기 위해 이를 출시했습니다. 피드백이 있으시면 community.n8n.io에 가서 의견을 남겨주세요.\n",
      "MCP 서버를 추가했으니, 이제 호스트가 액세스할 수 있는 몇 가지 도구를 제공해야 합니다. 첫 번째 예시로 계산기 도구만 추가해 보겠습니다. LLM은 수학을 잘 하지 못하므로 도구를 사용하는 것이 좋습니다. 워크플로우를 저장하고 활성화시켜야 합니다.\n",
      "다음으로 Claude나 다른 MCP 호스트로 이동해야 합니다. 여기서는 Claude 데스크톱 앱을 사용하겠습니다. Claude 데스크톱 앱이 없다면, 필요한 것은 Claude 데스크톱 앱을 다운로드하고, Anthropic 계정이 있어야 합니다. 또한 Node.js가 설치되어 있어야 합니다. Claude 데스크톱은 현재 SSE를 통한 MCP 클라이언트와의 통신을 지원하지 않기 때문에(n8n 팀이 MCP 트리거에서 지원하는 것) 게이트웨이를 사용해야 합니다. 이 게이트웨이는 기본적으로 SSE 부분을 처리하고 Claude가 MCP 트리거와 통신할 수 있도록 합니다.\n",
      "Claude 데스크톱을 열면, 개발자 모드인지 확인하세요. 상단 메뉴에서 도움말 섹션을 찾아 개발자 모드 활성화를 클릭하세요. Claude에서 설정을 열고 개발자 섹션으로 이동한 다음 \"Edit Config\" 버튼을 클릭하세요. 이렇게 하면 폴더가 열리고 JSON 파일이 보일 것입니다. 이 파일을 텍스트 편집기로 열어주세요.\n",
      "이전에 MCP를 설정하지 않았다면 이 페이지는 비어 있을 것입니다. 여기에 몇 가지 내용을 붙여넣어야 합니다. 저는 여기서 Super Gateway를 사용하고 있습니다. 이것은 Claude가 SSE 프로토콜을 통해 MCP 트리거와 통신할 수 있게 해주는데, Claude가 현재 이를 기본적으로 지원하지 않기 때문입니다. 그래서 이것이 의존성입니다.\n",
      "하나의 팁은, 이것이 실패하거나 오류가 발생한다면, 이 명령을 터미널에서 수동으로 실행해보세요. 'npx super-gateway -s'를 입력하고 웹훅 URL을 전달하세요. 저는 이 부분에서 오류가 몇 번 발생했고, 실제로 Node.js 자체에 권한 오류가 있었습니다. 그것을 수정한 후 작동했습니다.\n",
      "여기 구조는 MCP 서버가 있고, 여러 개의 MCP 서버를 가질 수 있습니다. 이 경우 웹훅 URL을 붙여넣어야 합니다. 워크플로우로 돌아가서 웹훅 URL을 복사하고 여기에 붙여넣으세요. 저장한 다음, Claude를 재시작해야 합니다. Command+Q를 눌러 닫고 다시 엽니다.\n",
      "이제 한 개의 MCP 도구가 있고, 계산기 도구가 n8n 서버에서 제공되는 것을 볼 수 있습니다. 서버를 계산기라고 불렀을 수도 있지만, n8n 인스턴스가 있고 Claude에게 몇 가지 도구에 대한 액세스 권한을 부여한다고 가정해 보겠습니다. 이 도구는 계산기라고 합니다. 실행해 볼까요? \"50 * 10은 얼마인가요? 도구를 사용하세요.\"라고 질문합니다.\n",
      "허가 확인이 나타납니다. 사용할 인수를 보여주는 것까지 보여주니 흥미롭네요. 이 채팅에 대해 허용해 보겠습니다. 도구와 상호 작용하는 방식의 사용자 경험이 정말 멋지네요. 응답은 500입니다. 워크플로우에서 실행을 확인해 보겠습니다. 계산기를 열어보면 500이라는 응답이 보입니다. 쿼리는 50 * 10이었고 완벽합니다.\n",
      "이것은 단순한 계산기 도구이며, Claude에도 자체 계산기가 있을 텐데 그렇게 흥미롭지 않을 수 있습니다. 하지만 제가 보여드리려는 것은 이것을 설정하는 방법입니다. 실제로 이것을 만드는 데 약간의 시간이 걸렸고 몇 번의 좌절을 겪었기 때문에 여러분이 그런 경험을 피할 수 있기를 바랍니다.\n",
      "YouTube에서 이 영상을 보시는 분들을 위해 설명란에 이 코드 스니펫을 넣어두겠습니다. 다시 한번 언급하자면, 여기에 여러 도구를 추가할 수 있습니다. 다양한 벡터 저장소를 사용할 수 있으므로 Claude 데스크톱이나 다른 호스트가 다양한 앱 및 서비스와 연결되는 지식 RAG 어시스턴트가 될 수 있습니다.\n",
      "언급하고 싶은 정말 멋진 것 중 하나는 n8n 워크플로우 도구입니다. 이를 통해 MCP 호스트가 임의의 n8n 워크플로우와 상호 작용할 수 있습니다. 이 워크플로우 자체가 AI 에이전트일 수도 있고, 격리된 자체 호스팅 기업 정보일 수도 있습니다. 이것이 n8n에서 MCP 서버 트리거를 설정하는 방법입니다.\n",
      "이제 AI 에이전트에서 도구를 클릭하여 캔버스에 추가해 보겠습니다. MCP를 검색하면 MCP 클라이언트 도구가 보입니다. 이를 추가해 보겠습니다.\n",
      "맥락을 이해하기 위해, 이 경우 AI 에이전트는 호스트입니다. 호스트로서 클라이언트와 상호 작용하게 됩니다. 클라이언트에서는 MCP 서버에 대한 연결을 구성할 것입니다. 따라서 AI 에이전트는 호스트이며, 클라이언트를 사용하여 MCP 서버와 상호 작용합니다.\n",
      "방금 다른 워크플로우에서 MCP 트리거를 통해 MCP 서버를 구축했으니, 여기 AI 에이전트가 해당 계산기 도구와 상호 작용하도록 해보겠습니다. 더블 클릭하고 연결할 자격 증명을 추가합니다. 새로운 자격 증명을 만들고, 자격 증명 내부의 SSE 엔드포인트는 MCP 트리거에서 가져와야 합니다. MCP 트리거로 이동하여 프로덕션 URL을 복사하고 SSE 엔드포인트에 붙여넣은 다음 저장합니다.\n",
      "연결 테스트가 성공했습니다. 이제 설정이 완료되었으니 자격 증명을 닫겠습니다. MCP 서버는 여러 도구를 제공할 수 있으므로, 서버가 제공하는 모든 도구를 가져오거나 선택적으로 선택할 수 있습니다. 이 경우, 시스템과의 연결이 설정되었음을 알 수 있습니다. 계산기가 표시되니까요.\n",
      "이제 워크플로우 캔버스로 돌아가서 수동 채팅을 열어 테스트해 보겠습니다. \"15 * 10을 계산해보세요\"라고 요청합니다. 도구를 사용했네요. 로그를 확인해 보면 쿼리를 보냈고 응답을 받았습니다. 이제 트리거로 이동하여 실행을 확인해 보면 잘 실행되었음을 알 수 있습니다.\n",
      "이렇게 n8n 내의 AI 에이전트에서 MCP 서버와 함께 MCP 클라이언트를 설정하는 방법입니다.\n",
      "방금 MCP가 무엇인지, n8n에서 네이티브 MCP 서버를 설정하는 방법, 그리고 다른 MCP 서버와 연결할 수 있는 MCP 도구(MCP 클라이언트)를 설정하는 방법을 설명했습니다.\n",
      "다음 단계는 n8n 버전을 업데이트하고 즐겁게 사용해 보는 것입니다. 아직 n8n에 가입하지 않았다면, 제 쿠폰 코드 MAX50을 사용하세요. 클라우드 평가판 이후에 n8n 클라우드를 1년 동안 50% 할인받을 수 있습니다. n8n을 자체 호스팅할 수도 있지만, 클라우드 옵션을 선택하면 제 상사가 정말 기뻐할 것입니다.\n",
      "이 영상이 유용했기를 바랍니다. 더 보고 싶은 내용이나 제가 놓친 것 같은 점에 대해 피드백을 주세요. 이 주제에 대한 마지막 영상은 아닐 것 같습니다.\n",
      "시청해 주셔서 감사합니다. 정말 감사합니다. 저는 오리지널 플로우그래머 Max입니다. 이 영상을 시청해 주셔서 감사하며, 즐거운 플로잉 되세요!\n",
      "요약해줘\n",
      "n8n이 MCP(Model Context Protocol)를 네이티브로 지원하기 시작했습니다. MCP는 Anthropic이 만든 표준화된 통신 프로토콜로, LLM과 다른 시스템 간의 상호작용을 표준화하려는 시도입니다. 이를 통해 n8n 사용자는 MCP 서버와 클라이언트 노드를 활용해 다양한 도구와 서비스에 쉽게 접근할 수 있게 되었습니다. 특히, MCP 서버를 통해 n8n 워크플로우에 계산기 도구와 같은 다양한 기능을 추가할 수 있으며, MCP 클라이언트를 통해 AI 에이전트가 다양한 MCP 서버와 상호작용할 수 있\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.\"},\n",
    "    {\"role\": \"user\", \"content\": data + \"요약해줘\"}\n",
    "]\n",
    "\n",
    "# Use the chat template to format the conversation\n",
    "prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "# Tokenize and generate\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=128,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfed1d0c",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "EXAONE-3.5-2.4B-Instruct has been evaluated on various benchmarks. Here are some evaluation results:\n",
    "\n",
    "| Model | MT-Bench | LiveBench | Arena-Hard | AlpacaEval | IFEval | KoMT-Bench | LogicKor |\n",
    "|-------|----------|-----------|------------|------------|--------|------------|----------|\n",
    "| EXAONE 3.5 2.4B | 7.81 | 33.0 | 48.2 | 37.1 | 73.6 | 7.24 | 8.51 |\n",
    "| Qwen 2.5 3B | 7.21 | 25.7 | 26.4 | 17.4 | 60.8 | 5.68 | 5.21 |\n",
    "| Qwen 2.5 1.5B | 5.72 | 19.2 | 10.6 | 8.4 | 40.7 | 3.87 | 3.60 |\n",
    "| Llama 3.2 3B | 6.94 | 24.0 | 14.2 | 18.7 | 70.1 | 3.16 | 2.86 |\n",
    "| Gemma 2 2B | 7.20 | 20.0 | 19.1 | 29.1 | 50.5 | 4.83 | 5.29 |\n",
    "\n",
    "Full evaluation results can be found in the [technical report](https://arxiv.org/abs/2412.04862)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2197b64f",
   "metadata": {},
   "source": [
    "## Deployment Options\n",
    "\n",
    "EXAONE 3.5 models can be inferred in various frameworks:\n",
    "\n",
    "- TensorRT-LLM\n",
    "- vLLM\n",
    "- SGLang\n",
    "- llama.cpp\n",
    "- Ollama\n",
    "\n",
    "For quantized versions, the model is available in AWQ and several quantization types in GGUF format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2edf5",
   "metadata": {},
   "source": [
    "## Limitations and License\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- May occasionally generate inappropriate responses containing personal, harmful or biased information\n",
    "- Responses rely on statistics from training data, which can result in semantically or syntactically incorrect sentences\n",
    "- May not reflect the latest information, leading to false or contradictory responses\n",
    "\n",
    "### License\n",
    "\n",
    "The model is licensed under EXAONE AI Model License Agreement 1.1 - NC\n",
    "\n",
    "### Citation\n",
    "\n",
    "```\n",
    "@article{exaone-3.5,\n",
    "  title={EXAONE 3.5: Series of Large Language Models for Real-world Use Cases},\n",
    "  author={LG AI Research},\n",
    "  journal={arXiv preprint arXiv:https://arxiv.org/abs/2412.04862},\n",
    "  year={2024}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
