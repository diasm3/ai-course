{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPJenlrhihrh"
   },
   "source": [
    "# MNIST 실습\n",
    "\n",
    "이번에는 28x28 흑백 손글씨 이미지를 보고 0~9 사이의 숫자 중 어떤 숫자를 쓴 것인지 예측하는 문제를 실습합니다.\n",
    "이번 실습에서는 GPU를 활용할 것이기 때문에, 이전 챕터에서 Colab에서 GPU를 설정하는 방법을 따라해주시길 바랍니다.\n",
    "\n",
    "GPU를 설정했으면 library들을 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Using cached torchvision-0.21.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: torch==2.6.0 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torchvision) (2.6.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: setuptools in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/semyungpark/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
      "Using cached torchvision-0.21.0-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.21.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 6315,
     "status": "ok",
     "timestamp": 1724212693193,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "6lXVfXoDtoQh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pF-1zQvmiult"
   },
   "source": [
    "다음은 dataset을 준비합니다. 손글씨 dataset은 MNIST라는 유명한 dataset이 있습니다. 이 dataset은 `torchvision`에서 제공하고 있으며, 다음과 같이 다운로드 받을 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22007,
     "status": "ok",
     "timestamp": 1724212718261,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "KAYlqPaRt6ti",
    "outputId": "67e51d96-85b4-4129-efc8-7938006afdd4"
   },
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터 로드\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-GqH7_ZjVMs"
   },
   "source": [
    "MNIST는 손글씨 사진과 어떤 숫자를 의미하는지에 대한 label의 pair들로 구성되어있습니다.\n",
    "이 때, 우리는 PyTorch model을 사용할 것이기 때문에 손글씨 사진들을 모두 tensor로 변환해야합니다.\n",
    "이러한 부가적인 변환들은 `torchvision.transforms`에서 제공하고 있으며, `torchvision.datasets.MNIST`에서 `transform` 인자로 받을 수 있습니다.\n",
    "우리는 단순히 사진을 tensor로 바꾸고 싶기 때문에 `transforms.ToTensor()` transformation을 넘겨줍니다.\n",
    "\n",
    "다음은 전체 data의 개수와 첫 번째 data를 출력한 결과입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 481
    },
    "executionInfo": {
     "elapsed": 601,
     "status": "ok",
     "timestamp": 1724212727291,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Zsp3sHmojyhT",
    "outputId": "3511bb75-e677-4fb8-894b-69e22c80f4bc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "torch.Size([1, 28, 28]) 7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x31659d850>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "print(len(trainset))\n",
    "print(trainset[0][0].shape, trainset[0][1])\n",
    "plt.imshow(trainset[0][0][0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udm8wDBsi301"
   },
   "source": [
    "첫 번째 출력결과를 통해 우리는 6만장의 손글씨 data가 있는 것을 알 수 있습니다.\n",
    "그리고 두 번째 출력결과를 통해 첫 번째 data의 shape은 (1, 28, 28)이고 5라는 숫자를 쓴 사진이라는 것을 알 수 있습니다.\n",
    "마지막으로 `plt.imshow`를 통해 visualize 했을 때 5라는 숫자가 나오는 것을 알 수 있습니다.\n",
    "\n",
    "다음은 SGD를 위해 dataset을 여러 개의 batch로 나누는 과정을 PyTorch로 구현한 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UxKu3kA2i5WH"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# 테스트 데이터 로더 생성\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4Qyrm4WlzWt"
   },
   "source": [
    "PyTorch에서는 `DataLoader`가 dataset을 인자로 받아 batch로 나눠줍니다.\n",
    "부가적으로 `batch_size`라는 인자를 통해 batch size를 받고 있으며, `shuffle`이라는 인자를 통해 data들을 섞을지 결정해줍니다.\n",
    "우리는 SGD가 완전 랜덤으로 batch를 구성해야 잘 동작하는 것을 알고 있기 때문에 `shuffle`에 `True`를 넘겨주고 있습니다.\n",
    "\n",
    "다음은 첫 번째 batch를 출력한 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1723366306519,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "_Hd7XxyAvVNz",
    "outputId": "185acb61-2ed7-4110-b325-c0e016ca60b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "print(images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0ZY5mDimRUK"
   },
   "source": [
    "`images`는 첫 번째 batch의 image들이고 `labels`는 첫 번째 batch의 label들입니다.\n",
    "위에서 batch size를 64로 설정했기 때문에 총 64개의 image와 label들이 있어야 합니다.\n",
    "실제 shape 출력 결과를 보면 그렇다는 것을 알 수 있습니다.\n",
    "\n",
    "다음은 (n, 1, 28, 28) shape의 image를 입력받아 0~9 사이의 정수 하나를 출력하는 3-layer MLP를 구현합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "OLOA-ZGTuVVG"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "    self.fc1 = nn.Linear(28*28, 512)\n",
    "    self.fc2 = nn.Linear(512, 256)\n",
    "    self.fc3 = nn.Linear(256, 10)  # 출력: 10개 클래스\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = x.view(-1, 28*28)  # flatten\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = self.fc3(x)  # 마지막에는 ReLU 제거, softmax는 CrossEntropyLoss에 포함됨\n",
    "\n",
    "    return x\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "\n",
    "# model = Model(28 * 28 * 1, 1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tag1rNcwmnAr"
   },
   "source": [
    "이전의 2-layer MLP와 유사한 형태임을 알 수 있습니다.\n",
    "여기서 특이사항은 `forward`의 첫 번째 줄에 `torch.flatten`을 사용한다는 것입니다.\n",
    "`Linear`는 이전에도 봤다시피 (n, d) 형태의 shape을 입력받습니다.\n",
    "이미지는 (n, 1, 28, 28)이기 때문에 (n, 1 * 28 * 28)로 shape을 변환해야 선형 함수에 입력으로 주어줄 수 있게 됩니다.\n",
    "이 역할을 수행하는 것이 바로 `torch.flatten`입니다.\n",
    "우리는 첫 번째 shape인 n을 보존할 것이기 때문에 flatten할 차원은 `start_dim=1`로 넘겨주게 됩니다.\n",
    "\n",
    "다음은 gradient descent를 수행해줄 optimizer를 구현하는 모습입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ypS0TcOlvBhZ"
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import torch\n",
    "\n",
    "# M1/M2/M3 맥북에서 MPS 사용할 수 있게 장치 설정\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "# 모델 생성 예시 (예: 간단한 linear model)\n",
    "model = torch.nn.Linear(10, 1)\n",
    "\n",
    "# 모델을 적절한 장치로 이동 (mps or cpu)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer 설정\n",
    "optimizer = SGD(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PooeOKMnRfZ"
   },
   "source": [
    "이전 코드와 거의 똑같습니다. 다른 점은 `model.to('cuda')` 코드를 통해 우리가 구현한 model을 GPU로 옮긴 것입니다.\n",
    "MNIST 부터는 모델과 data가 커지면서 훨씬 많은 행렬 연산이 이루어지기 때문에 GPU를 활용하는 것이 빠릅니다.\n",
    "\n",
    "다음은 model을 MNIST에 학습하는 코드입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 786155,
     "status": "ok",
     "timestamp": 1723367093025,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "TryX1hewvNiB",
    "outputId": "4e4a9282-d49a-40ff-c26c-d00fb7c0af79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Loss: 4776.3927\n",
      "Epoch   1 | Loss: 3882.8116\n",
      "Epoch   2 | Loss: 3683.8231\n",
      "Epoch   3 | Loss: 3554.4366\n",
      "Epoch   4 | Loss: 3462.0066\n",
      "Epoch   5 | Loss: 3391.9133\n",
      "Epoch   6 | Loss: 3336.7328\n",
      "Epoch   7 | Loss: 3292.7889\n",
      "Epoch   8 | Loss: 3258.9149\n",
      "Epoch   9 | Loss: 3230.8969\n",
      "Epoch  10 | Loss: 3208.7435\n",
      "Epoch  11 | Loss: 3188.2257\n",
      "Epoch  12 | Loss: 3173.3765\n",
      "Epoch  13 | Loss: 3159.9602\n",
      "Epoch  14 | Loss: 3149.4031\n",
      "Epoch  15 | Loss: 3139.6461\n",
      "Epoch  16 | Loss: 3130.6912\n",
      "Epoch  17 | Loss: 3122.9681\n",
      "Epoch  18 | Loss: 3118.3501\n",
      "Epoch  19 | Loss: 3111.8574\n",
      "Epoch  20 | Loss: 3107.4701\n",
      "Epoch  21 | Loss: 3103.2947\n",
      "Epoch  22 | Loss: 3099.3515\n",
      "Epoch  23 | Loss: 3095.2382\n",
      "Epoch  24 | Loss: 3092.0698\n",
      "Epoch  25 | Loss: 3089.6642\n",
      "Epoch  26 | Loss: 3087.2421\n",
      "Epoch  27 | Loss: 3084.0637\n",
      "Epoch  28 | Loss: 3081.1601\n",
      "Epoch  29 | Loss: 3079.9851\n",
      "Epoch  30 | Loss: 3077.1580\n",
      "Epoch  31 | Loss: 3076.0704\n",
      "Epoch  32 | Loss: 3074.8589\n",
      "Epoch  33 | Loss: 3072.7148\n",
      "Epoch  34 | Loss: 3070.7419\n",
      "Epoch  35 | Loss: 3069.8210\n",
      "Epoch  36 | Loss: 3068.0681\n",
      "Epoch  37 | Loss: 3067.5416\n",
      "Epoch  38 | Loss: 3065.9548\n",
      "Epoch  39 | Loss: 3064.7287\n",
      "Epoch  40 | Loss: 3063.6436\n",
      "Epoch  41 | Loss: 3061.7164\n",
      "Epoch  42 | Loss: 3060.3555\n",
      "Epoch  43 | Loss: 3060.7391\n",
      "Epoch  44 | Loss: 3059.4717\n",
      "Epoch  45 | Loss: 3060.6462\n",
      "Epoch  46 | Loss: 3057.4582\n",
      "Epoch  47 | Loss: 3057.1721\n",
      "Epoch  48 | Loss: 3056.4283\n",
      "Epoch  49 | Loss: 3055.1390\n",
      "Epoch  50 | Loss: 3054.2562\n",
      "Epoch  51 | Loss: 3053.8191\n",
      "Epoch  52 | Loss: 3053.7151\n",
      "Epoch  53 | Loss: 3051.3620\n",
      "Epoch  54 | Loss: 3051.8150\n",
      "Epoch  55 | Loss: 3050.6388\n",
      "Epoch  56 | Loss: 3051.4106\n",
      "Epoch  57 | Loss: 3049.8872\n",
      "Epoch  58 | Loss: 3050.1831\n",
      "Epoch  59 | Loss: 3049.1551\n",
      "Epoch  60 | Loss: 3048.5975\n",
      "Epoch  61 | Loss: 3048.6046\n",
      "Epoch  62 | Loss: 3047.0634\n",
      "Epoch  63 | Loss: 3046.8632\n",
      "Epoch  64 | Loss: 3048.2086\n",
      "Epoch  65 | Loss: 3046.4490\n",
      "Epoch  66 | Loss: 3045.5503\n",
      "Epoch  67 | Loss: 3045.8313\n",
      "Epoch  68 | Loss: 3045.6653\n",
      "Epoch  69 | Loss: 3045.0257\n",
      "Epoch  70 | Loss: 3044.5775\n",
      "Epoch  71 | Loss: 3043.5812\n",
      "Epoch  72 | Loss: 3044.3784\n",
      "Epoch  73 | Loss: 3043.4797\n",
      "Epoch  74 | Loss: 3042.4851\n",
      "Epoch  75 | Loss: 3042.7076\n",
      "Epoch  76 | Loss: 3042.0962\n",
      "Epoch  77 | Loss: 3041.6919\n",
      "Epoch  78 | Loss: 3042.3688\n",
      "Epoch  79 | Loss: 3041.2217\n",
      "Epoch  80 | Loss: 3041.3222\n",
      "Epoch  81 | Loss: 3040.4199\n",
      "Epoch  82 | Loss: 3038.8832\n",
      "Epoch  83 | Loss: 3041.5708\n",
      "Epoch  84 | Loss: 3040.2748\n",
      "Epoch  85 | Loss: 3040.0560\n",
      "Epoch  86 | Loss: 3040.1983\n",
      "Epoch  87 | Loss: 3039.2598\n",
      "Epoch  88 | Loss: 3039.2241\n",
      "Epoch  89 | Loss: 3039.1631\n",
      "Epoch  90 | Loss: 3038.2369\n",
      "Epoch  91 | Loss: 3038.4699\n",
      "Epoch  92 | Loss: 3036.9564\n",
      "Epoch  93 | Loss: 3037.6572\n",
      "Epoch  94 | Loss: 3037.1728\n",
      "Epoch  95 | Loss: 3038.3168\n",
      "Epoch  96 | Loss: 3038.0290\n",
      "Epoch  97 | Loss: 3036.7935\n",
      "Epoch  98 | Loss: 3038.0961\n",
      "Epoch  99 | Loss: 3036.1489\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "# MPS 또는 CPU 자동 선택\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 예시 모델\n",
    "model = nn.Linear(784, 1)\n",
    "model = model.to(device)\n",
    "\n",
    "# 옵티마이저\n",
    "optimizer = SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# 에폭 루프\n",
    "for epoch in range(100):\n",
    "    total_loss = 0.\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        \n",
    "        # 입력을 device로 이동 (중요!)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Flatten (MNIST 같은 경우)\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "\n",
    "        model.zero_grad()\n",
    "        preds = model(inputs)\n",
    "        loss = (preds[:, 0] - labels).pow(2).mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch:3d} | Loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRDcptrqniX7"
   },
   "source": [
    "출력 결과를 보면 잘 수렴하는 것을 볼 수 있습니다.\n",
    "이전 구현과 다른 점은 다음 두 가지입니다.\n",
    "- `for data in trainloader`를 통해 batch들을 iterate하면서 model을 학습합니다.\n",
    "- `inputs, labels = inputs.to('cuda'), labels.to('cuda')`를 통해 model의 입력으로 들어가는 tensor들을 GPU로 보냅니다.\n",
    "\n",
    "마지막으로 첫 번째 data에 대한 예측 결과를 살펴봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1723367093026,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "Zct0ssSKwjt1",
    "outputId": "4a8d7062-1a43-4522-895e-cfebb9c02cdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.2696]], device='mps:0')\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "\n",
    "# 입력 이미지 (1, 28, 28) → (1, 1, 28, 28)\n",
    "x = trainset[idx][0][None]  \n",
    "\n",
    "# MPS 또는 CPU로 전송\n",
    "x = x.to(device)\n",
    "\n",
    "# flatten 해줘야 Linear(784, 1)에 들어갈 수 있어\n",
    "x = x.view(x.size(0), -1)  # (1, 784)\n",
    "\n",
    "# 모델도 device로 옮겨져 있어야 함!\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(model(x))  # 예측값 출력\n",
    "\n",
    "print(trainset[idx][1])  # 실제 레이블"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qzgSOrMsoZ2z"
   },
   "source": [
    "여기서 idx를 조정하여 다른 data에 대한 출력 결과도 볼 수 있습니다.\n",
    "예측 결과를 보시면 아직 성능이 그렇게 좋지 않은 것을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "d6M4SpWQoML9"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (28x28 and 784x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m     acc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (labels \u001b[38;5;241m==\u001b[39m preds)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m acc \u001b[38;5;241m/\u001b[39m cnt\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43maccuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m, in \u001b[0;36maccuracy\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m      8\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 10\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(preds, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m cnt \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Documents/b2b/data_analyze/.conda/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (28x28 and 784x1)"
     ]
    }
   ],
   "source": [
    "# 정확도 측정 함수 추가\n",
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "    preds = model(inputs)\n",
    "    preds = torch.argmax(preds, dim=-1)\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt\n",
    "\n",
    "\n",
    "print(accuracy(model, testloader))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOz7vBAsZ4WYLph4xp9q2ee",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
