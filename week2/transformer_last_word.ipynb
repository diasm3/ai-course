{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터셋 로드 중...\n",
      "토크나이저 로드 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/semyungpark/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader 설정 중...\n",
      "모델 초기화 중...\n",
      "사용 디바이스: cpu\n",
      "학습 시작...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 343\u001b[39m\n\u001b[32m    341\u001b[39m outputs = model(inputs)\n\u001b[32m    342\u001b[39m loss = loss_fn(outputs, labels)\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    344\u001b[39m optimizer.step()\n\u001b[32m    346\u001b[39m total_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/mcp/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/mcp/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/envs/mcp/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 데이터셋 로드 - 전체 데이터셋 사용\n",
    "print(\"데이터셋 로드 중...\")\n",
    "train_ds = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "test_ds = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
    "\n",
    "# 토크나이저 로드\n",
    "print(\"토크나이저 로드 중...\")\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')\n",
    "\n",
    "# 새로운 collate_fn 구현 - 마지막 단어 예측용\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_len = 512  # 증가된 최대 길이\n",
    "    texts, labels = [], []\n",
    "    for row in batch:\n",
    "        # 마지막 단어를 label로 사용 (-3 위치의 token)\n",
    "        labels.append(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[-3])\n",
    "        # 마지막 단어를 제외한 나머지를 입력으로 사용\n",
    "        texts.append(torch.LongTensor(tokenizer(row['text'], truncation=True, max_length=max_len).input_ids[:-3]))\n",
    "    \n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.LongTensor(labels)\n",
    "    \n",
    "    return texts, labels\n",
    "\n",
    "# DataLoader 설정\n",
    "print(\"DataLoader 설정 중...\")\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=32, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=32, shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Positional Encoding 함수\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, None], np.arange(d_model)[None, :], d_model)\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "    \n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "# Self-Attention 구현\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.wq = nn.Linear(input_dim, d_model)\n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        score = torch.matmul(q, k.transpose(-1, -2))  # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "        score = score / sqrt(self.d_model)\n",
    "        \n",
    "        if mask is not None:\n",
    "            score = score + (mask * -1e9)\n",
    "        \n",
    "        score = self.softmax(score)\n",
    "        score = self.dropout(score)  # Attention dropout 추가\n",
    "        result = torch.matmul(score, v)\n",
    "        result = self.dense(result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Transformer Layer 구현\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "        \n",
    "        self.sa = SelfAttention(input_dim, d_model, dropout_rate)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, dff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),  # FFN Dropout 추가\n",
    "            nn.Linear(dff, d_model)\n",
    "        )\n",
    "        \n",
    "        # Layer Normalization 추가\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        # Attention 블록 (residual connection 추가)\n",
    "        attn_output = self.sa(x, mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        out1 = self.norm1(x + attn_output)  # Add & Norm\n",
    "        \n",
    "        # Feed Forward 블록 (residual connection 추가)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout(ffn_output)\n",
    "        out2 = self.norm2(out1 + ffn_output)  # Add & Norm\n",
    "        \n",
    "        return out2\n",
    "\n",
    "# 텍스트 분류 모델 수정 - 마지막 단어 예측용\n",
    "class LastWordPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(positional_encoding(512, d_model), requires_grad=False)  # 512로 변경\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.layers = nn.ModuleList([TransformerLayer(d_model, d_model, dff, dropout_rate) for _ in range(n_layers)])\n",
    "        \n",
    "        # 출력 레이어 변경 - 단일 값 대신 전체 어휘에 대한 예측 제공\n",
    "        self.classifier = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mask = (x == tokenizer.pad_token_id)\n",
    "        mask = mask[:, None, :]\n",
    "        seq_len = x.shape[1]\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "        x = self.dropout(x)  # 임베딩 후 dropout 추가\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        \n",
    "        # 마지막 토큰의 표현을 사용하여 다음 단어 예측\n",
    "        x = x[:, -1]  # 마지막 위치의 임베딩 사용\n",
    "        x = self.classifier(x)  # 어휘 크기로 변환\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Perplexity 계산 함수\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)  # [batch_size, vocab_size]\n",
    "            \n",
    "            # CrossEntropyLoss는 log-softmax를 포함하므로 NLL 계산\n",
    "            loss = nn.CrossEntropyLoss(reduction='sum')(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_tokens += labels.size(0)\n",
    "    \n",
    "    # Perplexity = exp(평균 NLL)\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "# Top-K 정확도 측정 함수\n",
    "def top_k_accuracy(model, dataloader, k=5, device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.topk(outputs, k, dim=1)\n",
    "            \n",
    "            # 각 sample에 대해 top-k에 정답이 있는지 확인\n",
    "            for i in range(labels.size(0)):\n",
    "                if labels[i] in predicted[i]:\n",
    "                    correct += 1\n",
    "            total += labels.size(0)\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# 기본 정확도 계산 함수\n",
    "def accuracy(model, dataloader, device=None):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total\n",
    "\n",
    "# 모델 초기화 - 더 큰 모델\n",
    "print(\"모델 초기화 중...\")\n",
    "d_model = 256  # 원래 64\n",
    "n_layers = 4    # 원래 2\n",
    "dff = 512       # 원래 128\n",
    "dropout_rate = 0.1\n",
    "\n",
    "model = LastWordPredictor(len(tokenizer), d_model, n_layers, dff, dropout_rate)\n",
    "\n",
    "# 학습 설정\n",
    "lr = 0.001\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"사용 디바이스: {device}\")\n",
    "model = model.to(device)\n",
    "\n",
    "# 손실 함수 변경 - CrossEntropyLoss 사용 (다중 클래스 분류)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=1, verbose=True)\n",
    "\n",
    "# 결과 추적용 리스트\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'test_acc': [],\n",
    "    'train_top5': [],\n",
    "    'test_top5': [],\n",
    "    'train_ppl': [],\n",
    "    'test_ppl': []\n",
    "}\n",
    "\n",
    "# 그래프 그리기 함수\n",
    "def plot_metrics(history):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    \n",
    "    # 손실\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(history['train_loss'])\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # 정확도\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['test_acc'], label='Test Accuracy')\n",
    "    plt.plot(history['train_top5'], label='Train Top-5 Accuracy')\n",
    "    plt.plot(history['test_top5'], label='Test Top-5 Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Perplexity\n",
    "    plt.subplot(3, 1, 3)\n",
    "    plt.plot(history['train_ppl'], label='Train Perplexity')\n",
    "    plt.plot(history['test_ppl'], label='Test Perplexity')\n",
    "    plt.title('Perplexity')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 예측 함수\n",
    "def predict_next_word(model, text, tokenizer, device):\n",
    "    # 텍스트 토큰화\n",
    "    tokens = tokenizer(text, truncation=True, max_length=512).input_ids\n",
    "    input_tensor = torch.LongTensor([tokens[:-3]]).to(device)\n",
    "    \n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        \n",
    "    # 상위 5개 가능한 다음 단어\n",
    "    probs, indices = torch.topk(torch.softmax(output[0], dim=0), 5)\n",
    "    next_words = [tokenizer.decode([idx.item()]) for idx in indices]\n",
    "    probs = probs.cpu().numpy()\n",
    "    \n",
    "    return list(zip(next_words, probs))\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"모델이 {path}에 저장되었습니다.\")\n",
    "\n",
    "# 모델 불러오기 함수\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"모델이 {path}에서 로드되었습니다.\")\n",
    "    return model\n",
    "\n",
    "# 모델 학습\n",
    "print(\"학습 시작...\")\n",
    "n_epochs = 10  # 5에서 증가\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # 순전파, 역전파, 최적화\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        batch_count += 1\n",
    "        \n",
    "        # 일정 간격으로 진행 상황 출력\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    # 평균 손실 계산\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    # 에폭마다 정확도 평가\n",
    "    train_acc = accuracy(model, train_loader, device)\n",
    "    test_acc = accuracy(model, test_loader, device)\n",
    "    \n",
    "    # Top-5 정확도 계산\n",
    "    top5_train = top_k_accuracy(model, train_loader, 5, device)\n",
    "    top5_test = top_k_accuracy(model, test_loader, 5, device)\n",
    "    \n",
    "    # Perplexity 계산\n",
    "    train_ppl = calculate_perplexity(model, train_loader, device)\n",
    "    test_ppl = calculate_perplexity(model, test_loader, device)\n",
    "    \n",
    "    # 결과 저장\n",
    "    history['train_loss'].append(avg_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['test_acc'].append(test_acc)\n",
    "    history['train_top5'].append(top5_train)\n",
    "    history['test_top5'].append(top5_test)\n",
    "    history['train_ppl'].append(train_ppl)\n",
    "    history['test_ppl'].append(test_ppl)\n",
    "    \n",
    "    # 결과 출력\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "    print(f\"Train Top-5: {top5_train:.4f} | Test Top-5: {top5_test:.4f}\")\n",
    "    print(f\"Train PPL: {train_ppl:.2f} | Test PPL: {test_ppl:.2f}\")\n",
    "    \n",
    "    # 학습률 조정\n",
    "    scheduler.step(test_ppl)\n",
    "    \n",
    "    # 최고 성능 모델 저장\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "        save_model(model, '/Users/semyungpark/Documents/homework/week2/best_model.pt')\n",
    "    \n",
    "    # 그래프 업데이트\n",
    "    clear_output(wait=True)\n",
    "    plot_metrics(history)\n",
    "    \n",
    "# 최종 모델 저장\n",
    "save_model(model, '/Users/semyungpark/Documents/homework/week2/final_model.pt')\n",
    "\n",
    "# 모델 예시 테스트\n",
    "test_sentences = [\n",
    "    \"I really enjoyed the movie because it was\",\n",
    "    \"The main character was portrayed as a\",\n",
    "    \"The plot of the story revolves around\",\n",
    "    \"In conclusion, this film is definitely\"\n",
    "]\n",
    "\n",
    "print(\"\\n예측 테스트:\")\n",
    "for sentence in test_sentences:\n",
    "    predictions = predict_next_word(model, sentence, tokenizer, device)\n",
    "    print(f\"\\n입력: {sentence}\")\n",
    "    for word, prob in predictions:\n",
    "        print(f\"  - {word} ({prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png][def]\n",
    "\n",
    "[def]: attachment:image.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
